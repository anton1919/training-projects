{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача: обучить нейронную сеть генерировать новые ингридиенты для зелий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('.../Potions.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Known ingredients</th>\n",
       "      <th>Effect</th>\n",
       "      <th>Characteristics</th>\n",
       "      <th>Difficulty level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ageing Potion</td>\n",
       "      <td>Newt spleens , Bananas</td>\n",
       "      <td>Ages drinker temporarily</td>\n",
       "      <td>Green</td>\n",
       "      <td>Advanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amortentia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Love Potion that causes a powerful infatuation...</td>\n",
       "      <td>Mother-of-pearl sheen, Spiralling steam, Scent...</td>\n",
       "      <td>Advanced</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Antidote to Veritaserum</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Counters the effect of Veritaserum</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Babbling Beverage</td>\n",
       "      <td>Valerian sprigs, Aconite, Dittany</td>\n",
       "      <td>Causes the drinker to speak nonsense</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Baruffio's Brain Elixir</td>\n",
       "      <td>Leaping Toadstools, Frog Brains, Runespoor egg...</td>\n",
       "      <td>Allegedly increases one's brain power</td>\n",
       "      <td>Green in colour</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Befuddlement Draught</td>\n",
       "      <td>Scurvy grass, Lovage, Sneezewort</td>\n",
       "      <td>Recklessness</td>\n",
       "      <td>Dark green in colour</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>First Love Beguiling Bubbles</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Causes the drinker to become infatuated with t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Fire Protection Potion</td>\n",
       "      <td>Bursting mushrooms, Salamander blood, Wartcap ...</td>\n",
       "      <td>Protects drinker from fire</td>\n",
       "      <td>Purple or Black</td>\n",
       "      <td>Beginner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Tolipan Blemish Blitzer</td>\n",
       "      <td>Dragon claw</td>\n",
       "      <td>Treats acne</td>\n",
       "      <td>White in colour, Thick paste consistency</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Blood-Replenishing Potion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Replenishes lost blood</td>\n",
       "      <td>Dark red in colour</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Name  \\\n",
       "0                 Ageing Potion   \n",
       "1                    Amortentia   \n",
       "2       Antidote to Veritaserum   \n",
       "3             Babbling Beverage   \n",
       "4       Baruffio's Brain Elixir   \n",
       "5          Befuddlement Draught   \n",
       "6  First Love Beguiling Bubbles   \n",
       "7        Fire Protection Potion   \n",
       "8       Tolipan Blemish Blitzer   \n",
       "9     Blood-Replenishing Potion   \n",
       "\n",
       "                                   Known ingredients  \\\n",
       "0                             Newt spleens , Bananas   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                  Valerian sprigs, Aconite, Dittany   \n",
       "4  Leaping Toadstools, Frog Brains, Runespoor egg...   \n",
       "5                   Scurvy grass, Lovage, Sneezewort   \n",
       "6                                                NaN   \n",
       "7  Bursting mushrooms, Salamander blood, Wartcap ...   \n",
       "8                                        Dragon claw   \n",
       "9                                                NaN   \n",
       "\n",
       "                                              Effect  \\\n",
       "0                           Ages drinker temporarily   \n",
       "1  Love Potion that causes a powerful infatuation...   \n",
       "2                 Counters the effect of Veritaserum   \n",
       "3               Causes the drinker to speak nonsense   \n",
       "4              Allegedly increases one's brain power   \n",
       "5                                       Recklessness   \n",
       "6  Causes the drinker to become infatuated with t...   \n",
       "7                         Protects drinker from fire   \n",
       "8                                        Treats acne   \n",
       "9                             Replenishes lost blood   \n",
       "\n",
       "                                     Characteristics Difficulty level  \n",
       "0                                              Green         Advanced  \n",
       "1  Mother-of-pearl sheen, Spiralling steam, Scent...         Advanced  \n",
       "2                                                NaN              NaN  \n",
       "3                                                NaN              NaN  \n",
       "4                                    Green in colour              NaN  \n",
       "5                               Dark green in colour              NaN  \n",
       "6                                                NaN              NaN  \n",
       "7                                    Purple or Black         Beginner  \n",
       "8           White in colour, Thick paste consistency              NaN  \n",
       "9                                 Dark red in colour              NaN  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предобработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [name.lower() for name in df.columns] # смена регистра в названии столбцов для удобства обращения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingridients = df['known ingredients'].tolist() # преобразование датафрейма в список"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingridients = [i.split(',') for i in ingridients if isinstance(i, str)] # очищение от Nanи запятых"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingridients = [i.strip() for lst in ingridients for i in lst] # превращение в одни массив"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingridients = [i.lower() for i in ingridients] # приведение к нижнему регистру"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingridients = [[c for c in i if c.isalpha() or c.isspace()] for i in ingridients] # разбиение на буквы и пробелы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['n', 'e', 'w', 't', ' ', 's', 'p', 'l', 'e', 'e', 'n', 's'],\n",
       " ['b', 'a', 'n', 'a', 'n', 'a', 's'],\n",
       " ['v', 'a', 'l', 'e', 'r', 'i', 'a', 'n', ' ', 's', 'p', 'r', 'i', 'g', 's'],\n",
       " ['a', 'c', 'o', 'n', 'i', 't', 'e'],\n",
       " ['d', 'i', 't', 't', 'a', 'n', 'y']]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingridients[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовка данных для подачи в нейросеть (оцифровывание строчных символов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = set('abcdefghijklmnopqrstuvwxyz ')\n",
    "index_to_char = ['none'] + [w for w in chars]\n",
    "char_to_index = {w: i for i, w in enumerate(index_to_char)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('none', 0), ('c', 1), ('u', 2), ('w', 3), ('q', 4), ('r', 5), ('z', 6), ('o', 7), ('d', 8), ('s', 9), ('f', 10), ('g', 11), ('a', 12), ('p', 13), ('y', 14), ('i', 15), ('b', 16), ('l', 17), ('e', 18), ('x', 19), ('t', 20), ('h', 21), ('k', 22), ('n', 23), ('j', 24), ('m', 25), ('v', 26), (' ', 27)])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_index.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перевод ингридиентов из строкового вида в числовой для подачи в нейросеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Определение максимально длинного названия ингридиента\n",
    "# чтобы выбрать количество нейронов в первом слое нейросети\n",
    "max_len = max([len(i) for i in ingridients])\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.zeros((len(ingridients), max_len), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ingridients)):\n",
    "    for j, w in enumerate(ingridients[i]):\n",
    "        if j >= max_len:\n",
    "            break\n",
    "        X[i, j] = char_to_index.get(w, char_to_index['none'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[23, 18,  3, 20, 27,  9, 13, 17, 18, 18, 23,  9,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "          0,  0,  0]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определение нейросети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.word_embeddings = torch.nn.Embedding(len(index_to_char), 28)\n",
    "        self.gru = torch.nn.GRU(28, 128, batch_first=True)\n",
    "        self.hidden2tag = torch.nn.Linear(128, len(index_to_char))\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        embeds = self.word_embeddings(sentences)\n",
    "        gru_out, state = self.gru(embeds)\n",
    "        tag_space = self.hidden2tag(gru_out.reshape(-1, 128))\n",
    "        return tag_space.reshape(sentences.shape[0], sentences.shape[1], -1), state\n",
    "\n",
    "    def forward_state(self, sentences, state):\n",
    "        embeds = self.word_embeddings(sentences)\n",
    "        gru_out, state = self.gru(embeds, state)\n",
    "        tag_space = self.hidden2tag(gru_out.reshape(-1, 128))\n",
    "        return tag_space.reshape(sentences.shape[0], sentences.shape[1], -1), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 57, 28])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(X[:1])[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ingridient(start_letter='a'):\n",
    "    sentence = [start_letter]\n",
    "    state = None\n",
    "    for i in range(max_len):\n",
    "        X = torch.Tensor([[char_to_index[sentence[i]]]]).type(torch.long)\n",
    "        if i == 0:\n",
    "            result, state = model.forward(X)\n",
    "        else:\n",
    "            result, state = model.forward_state(X, state)\n",
    "        prediction = result[0, -1, :]\n",
    "        index_of_prediction = prediction.argmax()\n",
    "        if i >= len(sentence) - 1:\n",
    "            if index_of_prediction == 0:\n",
    "                break\n",
    "        sentence.append(index_to_char[index_of_prediction])\n",
    "        \n",
    "    print(''.join(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sla bday\n"
     ]
    }
   ],
   "source": [
    "generate_ingridient('s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:, :-1]\n",
    "Y_train = X[:, 1:].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 0. Time: 0.275. Train loss: 3.172\n",
      "a\n",
      "Ep: 1. Time: 0.238. Train loss: 5.533\n",
      "a\n",
      "Ep: 2. Time: 0.237. Train loss: 16.437\n",
      "aocsc b\n",
      "Ep: 3. Time: 0.249. Train loss: 7.011\n",
      "auooogiaoooogiauuuugiauuuuuuuuuug ieloauooooooooooooooooao\n",
      "Ep: 4. Time: 0.262. Train loss: 8.266\n",
      "aooooooooooooooooooooooooooooooooooooooooooooooooooooooooo\n",
      "Ep: 5. Time: 0.258. Train loss: 8.513\n",
      "admesooooooooooooooooooooooooooooooooooooooooooooooooooooo\n",
      "Ep: 6. Time: 0.271. Train loss: 8.415\n",
      "aooooooooooooooooooooooooooooooooooooooooooooooooooooooooo\n",
      "Ep: 7. Time: 0.270. Train loss: 8.729\n",
      "aooooooooooooooooooooooooooooooooooooooooooooooooooooooooo\n",
      "Ep: 8. Time: 0.267. Train loss: 8.838\n",
      "adiooooooooooooooooooooooooooooooooooooooooooooooooooooooo\n",
      "Ep: 9. Time: 0.272. Train loss: 7.804\n",
      "awe\n",
      "Ep: 10. Time: 0.280. Train loss: 7.324\n",
      "awe\n",
      "Ep: 11. Time: 0.267. Train loss: 7.022\n",
      "ar\n",
      "Ep: 12. Time: 0.282. Train loss: 6.233\n",
      "an blspr\n",
      "Ep: 13. Time: 0.277. Train loss: 6.138\n",
      "an basbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb\n",
      "Ep: 14. Time: 0.286. Train loss: 5.644\n",
      "acscdedddddddddddddddddddddddddddddddddddddddddddddddddddd\n",
      "Ep: 15. Time: 0.290. Train loss: 5.415\n",
      "agsbnigine\n",
      "Ep: 16. Time: 0.282. Train loss: 5.581\n",
      "atdst ool oon ltm oono ool rol rof rowgdspicmj\n",
      "Ep: 17. Time: 0.300. Train loss: 6.284\n",
      "acrlw\n",
      "Ep: 18. Time: 0.286. Train loss: 5.628\n",
      "acshoooooooooooooooooooooooooooooooooooooooooooooooooooooo\n",
      "Ep: 19. Time: 0.294. Train loss: 5.483\n",
      "aclarofaia i     i                                        \n",
      "Ep: 20. Time: 0.280. Train loss: 5.342\n",
      "asyagsssssssssssssssssssssssssssssssssssssssssssssssssssss\n",
      "Ep: 21. Time: 0.300. Train loss: 5.471\n",
      "at\n",
      "Ep: 22. Time: 0.326. Train loss: 5.609\n",
      "at\n",
      "Ep: 23. Time: 0.307. Train loss: 5.395\n",
      "asin s tn\n",
      "Ep: 24. Time: 0.312. Train loss: 5.093\n",
      "agr\n",
      "Ep: 25. Time: 0.329. Train loss: 4.901\n",
      "agr\n",
      "Ep: 26. Time: 0.330. Train loss: 4.592\n",
      "as tt t t t t t t t t t t t t t t t t t t t t t t t t t t \n",
      "Ep: 27. Time: 0.336. Train loss: 4.380\n",
      "adariee\n",
      "Ep: 28. Time: 0.314. Train loss: 4.281\n",
      "adamy ingis\n",
      "Ep: 29. Time: 0.328. Train loss: 4.150\n",
      "avy\n",
      "Ep: 30. Time: 0.341. Train loss: 4.226\n",
      "adrr cuhny\n",
      "Ep: 31. Time: 0.334. Train loss: 4.100\n",
      "adrwe\n",
      "Ep: 32. Time: 0.355. Train loss: 4.108\n",
      "adrwk\n",
      "Ep: 33. Time: 0.362. Train loss: 3.792\n",
      "adr spientsntmwn\n",
      "Ep: 34. Time: 0.355. Train loss: 3.899\n",
      "ap wshmlllllllllllllllllllllllllllllllllllllllllllllllllll\n",
      "Ep: 35. Time: 0.353. Train loss: 3.660\n",
      "ap hsooollennfoooooooooooooooooooooooooooooooooooooooooooo\n",
      "Ep: 36. Time: 0.356. Train loss: 3.615\n",
      "anedrcfians\n",
      "Ep: 37. Time: 0.373. Train loss: 3.468\n",
      "aywny\n",
      "Ep: 38. Time: 0.367. Train loss: 3.481\n",
      "alerve\n",
      "Ep: 39. Time: 0.374. Train loss: 3.332\n",
      "alesl s w hmwmwmwmwmwmwmwmwmwmwmwmwmwmwmwmwmwmwmwmwmwmwmwm\n",
      "Ep: 40. Time: 0.366. Train loss: 3.265\n",
      "alesl s s  s s p\n",
      "Ep: 41. Time: 0.370. Train loss: 3.176\n",
      "at  h  alds s t thidn \n",
      "Ep: 42. Time: 0.359. Train loss: 3.073\n",
      "at r p\n",
      "Ep: 43. Time: 0.377. Train loss: 2.960\n",
      "at rp\n",
      "Ep: 44. Time: 0.361. Train loss: 2.843\n",
      "as\n",
      "Ep: 45. Time: 0.378. Train loss: 2.788\n",
      "a sp\n",
      "Ep: 46. Time: 0.379. Train loss: 2.663\n",
      "a spiercp\n",
      "Ep: 47. Time: 0.383. Train loss: 2.551\n",
      "an s\n",
      "Ep: 48. Time: 0.389. Train loss: 2.450\n",
      "an sp\n",
      "Ep: 49. Time: 0.363. Train loss: 2.352\n",
      "avleaillllllllllllllllllllllllllllllllllllllllllllllllllll\n",
      "Ep: 50. Time: 0.382. Train loss: 2.326\n",
      "avlan shentandd st s caladilllllllllllllllllllllllllllllll\n",
      "Ep: 51. Time: 0.372. Train loss: 2.314\n",
      "andrag ragriantgasbbsbwet ooromwwd illllllllllllllllllllll\n",
      "Ep: 52. Time: 0.378. Train loss: 2.336\n",
      "andntewed in coroooooooooooooooooooooooooooooooooooooooooo\n",
      "Ep: 53. Time: 0.392. Train loss: 2.261\n",
      "andnt \n",
      "Ep: 54. Time: 0.390. Train loss: 2.215\n",
      "andne ine eger on lace\n",
      "Ep: 55. Time: 0.361. Train loss: 2.145\n",
      "andne in otrsje ne ee ee of fgrincaddwdientt ttywoiggggggg\n",
      "Ep: 56. Time: 0.387. Train loss: 2.084\n",
      "andagrinff v\n",
      "Ep: 57. Time: 0.399. Train loss: 2.007\n",
      "anak\n",
      "Ep: 58. Time: 0.399. Train loss: 1.926\n",
      "anak\n",
      "Ep: 59. Time: 0.399. Train loss: 1.867\n",
      "anag s s s s s s s s s s s s s s s s s s s s s s s s s s s\n",
      "Ep: 60. Time: 0.392. Train loss: 1.759\n",
      "anag sigegs tobidd gircvr\n",
      "Ep: 61. Time: 0.392. Train loss: 1.727\n",
      "anag sin a sin cathlpple\n",
      "Ep: 62. Time: 0.408. Train loss: 1.670\n",
      "astabace rtin ca\n",
      "Ep: 63. Time: 0.415. Train loss: 1.639\n",
      "astabicatraplerigssitnt slsp\n",
      "Ep: 64. Time: 0.409. Train loss: 1.594\n",
      "ag rn tt\n",
      "Ep: 65. Time: 0.396. Train loss: 1.552\n",
      "ag slere f e\n",
      "Ep: 66. Time: 0.393. Train loss: 1.508\n",
      "ag slshla sls\n",
      "Ep: 67. Time: 0.378. Train loss: 1.466\n",
      "an tytupnee f e\n",
      "Ep: 68. Time: 0.360. Train loss: 1.407\n",
      "an ps of tinng\n",
      "Ep: 69. Time: 0.380. Train loss: 1.350\n",
      "ash easd ubicaranaga sh pien\n",
      "Ep: 70. Time: 0.385. Train loss: 1.322\n",
      "awge\n",
      "Ep: 71. Time: 0.409. Train loss: 1.300\n",
      "atarpood slusdd sntcurupre\n",
      "Ep: 72. Time: 0.408. Train loss: 1.240\n",
      "ashredzawe\n",
      "Ep: 73. Time: 0.377. Train loss: 1.222\n",
      "an icededillaeeedilllawentlapniouooooooooooooooooooooooooo\n",
      "Ep: 74. Time: 0.391. Train loss: 1.217\n",
      "ashredz w\n",
      "Ep: 75. Time: 0.403. Train loss: 1.151\n",
      "ashredz w\n",
      "Ep: 76. Time: 0.389. Train loss: 1.149\n",
      "and of lee gsqurimsasdwilsdia biayeldd oz mamawyawit\n",
      "Ep: 77. Time: 0.402. Train loss: 1.140\n",
      "andicedd lld b bicluningsougentdrn incotswat ageried\n",
      "Ep: 78. Time: 0.402. Train loss: 1.104\n",
      "ande otuce\n",
      "Ep: 79. Time: 0.393. Train loss: 1.074\n",
      "an spihn\n",
      "Ep: 80. Time: 0.390. Train loss: 1.031\n",
      "an ow spihne\n",
      "Ep: 81. Time: 0.406. Train loss: 1.034\n",
      "an ow spihne\n",
      "Ep: 82. Time: 0.401. Train loss: 1.032\n",
      "an spih\n",
      "Ep: 83. Time: 0.386. Train loss: 0.998\n",
      "an spih\n",
      "Ep: 84. Time: 0.389. Train loss: 1.012\n",
      "an rag orwgstllapopde\n",
      "Ep: 85. Time: 0.389. Train loss: 0.998\n",
      "an uspt\n",
      "Ep: 86. Time: 0.396. Train loss: 0.960\n",
      "andewsoineewescuman ang t\n",
      "Ep: 87. Time: 0.392. Train loss: 0.971\n",
      "anak\n",
      "Ep: 88. Time: 0.396. Train loss: 0.956\n",
      "ant ts\n",
      "Ep: 89. Time: 0.405. Train loss: 0.937\n",
      "anak\n",
      "Ep: 90. Time: 0.392. Train loss: 0.901\n",
      "at furmeese in twssp ntbailillllllllllllllllllllllllllllll\n",
      "Ep: 91. Time: 0.393. Train loss: 0.877\n",
      "at furmeese in twssywig shedteriedilawdte\n",
      "Ep: 92. Time: 0.403. Train loss: 0.890\n",
      "any tfurss\n",
      "Ep: 93. Time: 0.393. Train loss: 0.875\n",
      "andined tn\n",
      "Ep: 94. Time: 0.378. Train loss: 0.851\n",
      "andined szy tussit sls\n",
      "Ep: 95. Time: 0.405. Train loss: 0.836\n",
      "anak\n",
      "Ep: 96. Time: 0.402. Train loss: 0.821\n",
      "agof shasstoot ph\n",
      "Ep: 97. Time: 0.396. Train loss: 0.824\n",
      "anak\n",
      "Ep: 98. Time: 0.384. Train loss: 0.817\n",
      "anak\n",
      "Ep: 99. Time: 0.386. Train loss: 0.810\n",
      "andined szy gre\n",
      "Ep: 100. Time: 0.400. Train loss: 0.803\n",
      "an ouin claob bit\n",
      "Ep: 101. Time: 0.401. Train loss: 0.835\n",
      "anak\n",
      "Ep: 102. Time: 0.390. Train loss: 0.843\n",
      "an ousspr s spri sst\n",
      "Ep: 103. Time: 0.398. Train loss: 0.809\n",
      "an ous ot\n",
      "Ep: 104. Time: 0.412. Train loss: 0.818\n",
      "anak\n",
      "Ep: 105. Time: 0.394. Train loss: 0.870\n",
      "acupingess\n",
      "Ep: 106. Time: 0.390. Train loss: 0.897\n",
      "an oup e p e spt e\n",
      "Ep: 107. Time: 0.405. Train loss: 0.898\n",
      "aigs\n",
      "Ep: 108. Time: 0.400. Train loss: 0.860\n",
      "anak\n",
      "Ep: 109. Time: 0.400. Train loss: 0.908\n",
      "at  yurotureeesereeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee\n",
      "Ep: 110. Time: 0.397. Train loss: 0.885\n",
      "an spr ssprooss sned mandrtewbobb bkne sterined\n",
      "Ep: 111. Time: 0.384. Train loss: 0.825\n",
      "acuinggggggggggggggggggggggggggggggggggggggggggggggggggggg\n",
      "Ep: 112. Time: 0.408. Train loss: 0.857\n",
      "an spr ssh ononed er s s ss ss ss ss ss ss ss ss ss ss ss \n",
      "Ep: 113. Time: 0.397. Train loss: 0.811\n",
      "an spr s ononentangs\n",
      "Ep: 114. Time: 0.398. Train loss: 0.802\n",
      "agre\n",
      "Ep: 115. Time: 0.413. Train loss: 0.889\n",
      "an s spo ts\n",
      "Ep: 116. Time: 0.394. Train loss: 0.856\n",
      "an spred s sn ywrms\n",
      "Ep: 117. Time: 0.400. Train loss: 0.847\n",
      "arpivvvvvlavelggawsrigs\n",
      "Ep: 118. Time: 0.427. Train loss: 0.841\n",
      "at fuxshe\n",
      "Ep: 119. Time: 0.400. Train loss: 0.839\n",
      "an sprnced dacng \n",
      "Ep: 120. Time: 0.391. Train loss: 0.892\n",
      "awge wofbravye\n",
      "Ep: 121. Time: 0.399. Train loss: 0.911\n",
      "as opopof veeerededk\n",
      "Ep: 122. Time: 0.406. Train loss: 0.895\n",
      "antintunts\n",
      "Ep: 123. Time: 0.400. Train loss: 0.854\n",
      "an tantatats\n",
      "Ep: 124. Time: 0.395. Train loss: 0.838\n",
      "an scs\n",
      "Ep: 125. Time: 0.399. Train loss: 0.881\n",
      "anakrak a ywapplewed rwolbebebebeberekegroooorm m jrugeofr\n",
      "Ep: 126. Time: 0.387. Train loss: 0.954\n",
      "anake  tandabidanyee l n in lon hgobbbbbbbbbbbbbbbbbbbbbbb\n",
      "Ep: 127. Time: 0.383. Train loss: 0.962\n",
      "anakraklanyince\n",
      "Ep: 128. Time: 0.378. Train loss: 0.903\n",
      "asssssssssssssssssssssssssssssssssssssssssssssssssssssssss\n",
      "Ep: 129. Time: 0.387. Train loss: 1.008\n",
      "acuigggggggggggggggggggggggggggggggggggggggggggggggggggggg\n",
      "Ep: 130. Time: 0.364. Train loss: 0.998\n",
      "acuicwowowowowowowowowowowowowowowowowowowowowowowowowowow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 131. Time: 0.391. Train loss: 1.177\n",
      "at yotp\n",
      "Ep: 132. Time: 0.404. Train loss: 1.176\n",
      "ag mza ddddddddddddddddddddddddddddddddddddddddddddddddddd\n",
      "Ep: 133. Time: 0.391. Train loss: 1.158\n",
      "anakrakr\n",
      "Ep: 134. Time: 0.385. Train loss: 1.152\n",
      "anakrakr\n",
      "Ep: 135. Time: 0.398. Train loss: 1.052\n",
      "an waphpndywih\n",
      "Ep: 136. Time: 0.397. Train loss: 1.056\n",
      "a s s s s s s s s s sts\n",
      "Ep: 137. Time: 0.391. Train loss: 1.000\n",
      "ales\n",
      "Ep: 138. Time: 0.394. Train loss: 0.968\n",
      "anakrakr\n",
      "Ep: 139. Time: 0.401. Train loss: 0.968\n",
      "asptlontocpowsptohooneedmexwdentwddintcurat\n",
      "Ep: 140. Time: 0.406. Train loss: 0.948\n",
      "asprsywif wd f g ng ckent\n",
      "Ep: 141. Time: 0.397. Train loss: 0.876\n",
      "as on act fur age qh r egong caker\n",
      "Ep: 142. Time: 0.388. Train loss: 0.861\n",
      "andinersy\n",
      "Ep: 143. Time: 0.400. Train loss: 0.856\n",
      "ans\n",
      "Ep: 144. Time: 0.401. Train loss: 0.838\n",
      "agof orwopo jgom st  grass\n",
      "Ep: 145. Time: 0.394. Train loss: 0.821\n",
      "at  fa mamonomzy  fblsslsslslslslslslslslslslslslslslslsls\n",
      "Ep: 146. Time: 0.390. Train loss: 0.793\n",
      "at ye\n",
      "Ep: 147. Time: 0.403. Train loss: 0.799\n",
      "anakwgbbihomesce\n",
      "Ep: 148. Time: 0.399. Train loss: 0.759\n",
      "an waphsphhs\n",
      "Ep: 149. Time: 0.380. Train loss: 0.745\n",
      "an spcongof wowoney\n",
      "Ep: 150. Time: 0.389. Train loss: 0.754\n",
      "ale\n",
      "Ep: 151. Time: 0.402. Train loss: 0.709\n",
      "ale\n",
      "Ep: 152. Time: 0.397. Train loss: 0.713\n",
      "anakes\n",
      "Ep: 153. Time: 0.401. Train loss: 0.707\n",
      "at furs sousomzaweeden\n",
      "Ep: 154. Time: 0.396. Train loss: 0.692\n",
      "at odd on corooo\n",
      "Ep: 155. Time: 0.385. Train loss: 0.708\n",
      "an ageaienlon orn sinshormumyit\n",
      "Ep: 156. Time: 0.368. Train loss: 0.659\n",
      "asiteltggggggggggggggggggggggggggggggggggggggggggggggggggg\n",
      "Ep: 157. Time: 0.435. Train loss: 0.665\n",
      "anakerkwgg mstingvv\n",
      "Ep: 158. Time: 0.419. Train loss: 0.641\n",
      "anakerkwgoon mz\n",
      "Ep: 159. Time: 0.365. Train loss: 0.633\n",
      "an ushineshconi\n",
      "Ep: 160. Time: 0.402. Train loss: 0.618\n",
      "an usher oustin cloooooooooooooooooooooooooooooooooooooooo\n",
      "Ep: 161. Time: 0.390. Train loss: 0.621\n",
      "an usher ou sinshiced aigec\n",
      "Ep: 162. Time: 0.373. Train loss: 0.601\n",
      "acon hoess\n",
      "Ep: 163. Time: 0.388. Train loss: 0.597\n",
      "acuinggggggggggggggggggggggggggggggggggggggggggggggggggggg\n",
      "Ep: 164. Time: 0.380. Train loss: 0.585\n",
      "an srallllllllllllllllllllllllllllllllllllllllllllllllllll\n",
      "Ep: 165. Time: 0.400. Train loss: 0.571\n",
      "acuinggggggggggggggggggggggggggggggggggggggggggggggggggggg\n",
      "Ep: 166. Time: 0.385. Train loss: 0.565\n",
      "acuinggggggggggggggggggggggggggggggggggggggggggggggggggggg\n",
      "Ep: 167. Time: 0.386. Train loss: 0.555\n",
      "anakerkwgoonilonicall\n",
      "Ep: 168. Time: 0.398. Train loss: 0.545\n",
      "anaked on orooof filfxymit\n",
      "Ep: 169. Time: 0.392. Train loss: 0.534\n",
      "an urigs\n",
      "Ep: 170. Time: 0.368. Train loss: 0.537\n",
      "an ageliet sh\n",
      "Ep: 171. Time: 0.359. Train loss: 0.527\n",
      "an ageainged tng\n",
      "Ep: 172. Time: 0.377. Train loss: 0.521\n",
      "at fumingedd tin hurigs\n",
      "Ep: 173. Time: 0.394. Train loss: 0.520\n",
      "anaked piontinwt tlw ms\n",
      "Ep: 174. Time: 0.385. Train loss: 0.510\n",
      "an ag buriotigs\n",
      "Ep: 175. Time: 0.396. Train loss: 0.511\n",
      "arigs\n",
      "Ep: 176. Time: 0.398. Train loss: 0.509\n",
      "an ageaigs\n",
      "Ep: 177. Time: 0.386. Train loss: 0.499\n",
      "an ag burian ouriothig mus\n",
      "Ep: 178. Time: 0.366. Train loss: 0.512\n",
      "ales\n",
      "Ep: 179. Time: 0.369. Train loss: 0.522\n",
      "an urigs\n",
      "Ep: 180. Time: 0.381. Train loss: 0.510\n",
      "an aw as st wtiayer\n",
      "Ep: 181. Time: 0.382. Train loss: 0.515\n",
      "at ywofur oooooooooooooooooooooooooooooooooooooooooooooooo\n",
      "Ep: 182. Time: 0.405. Train loss: 0.540\n",
      "an usned siub bu b bunis\n",
      "Ep: 183. Time: 0.390. Train loss: 0.548\n",
      "an uras bbs\n",
      "Ep: 184. Time: 0.401. Train loss: 0.520\n",
      "alillywigelly\n",
      "Ep: 185. Time: 0.383. Train loss: 0.533\n",
      "an usned siub buniomsch rnt\n",
      "Ep: 186. Time: 0.395. Train loss: 0.554\n",
      "ashrientl sustoc qffxysw greesstygus\n",
      "Ep: 187. Time: 0.394. Train loss: 0.566\n",
      "ans\n",
      "Ep: 188. Time: 0.382. Train loss: 0.532\n",
      "an lsde\n",
      "Ep: 189. Time: 0.391. Train loss: 0.578\n",
      "at ywa\n",
      "Ep: 190. Time: 0.402. Train loss: 0.601\n",
      "an usnaker pionirnfesey\n",
      "Ep: 191. Time: 0.379. Train loss: 0.691\n",
      "an usur pion ored ras s s s s s s s s s s s s s s s s s s \n",
      "Ep: 192. Time: 0.387. Train loss: 0.691\n",
      "an ustw to t wanto tyur tyto tyur\n",
      "Ep: 193. Time: 0.401. Train loss: 0.677\n",
      "an uiggs\n",
      "Ep: 194. Time: 0.391. Train loss: 0.636\n",
      "alshelpopopoon pseewsp wown ouri shlalyacl ban srassssssss\n",
      "Ep: 195. Time: 0.386. Train loss: 0.636\n",
      "ales\n",
      "Ep: 196. Time: 0.384. Train loss: 0.631\n",
      "at ywager\n",
      "Ep: 197. Time: 0.385. Train loss: 0.594\n",
      "ashienocm\n",
      "Ep: 198. Time: 0.382. Train loss: 0.634\n",
      "an uiges\n",
      "Ep: 199. Time: 0.383. Train loss: 0.669\n",
      "an la\n"
     ]
    }
   ],
   "source": [
    "for ep in range(200):\n",
    "    start =time.time()\n",
    "    train_loss = 0\n",
    "    train_passed = 0\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    answers, _ = model.forward(X_train)\n",
    "    answers = answers.view(-1, len(index_to_char))\n",
    "    loss = criterion(answers, Y_train)\n",
    "    train_loss += loss.item()\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_passed += 1\n",
    "    \n",
    "    print('Ep: {}. Time: {:.3f}. Train loss: {:.3f}'.format(ep, time.time() - start,\n",
    "                                                            train_loss / train_passed))\n",
    "    generate_ingridient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вывод: в процессе запуска модели видно, что значение loss-функции снижается. Это может говорить о том, что модель обучается. Хотя модель не возвращает осознанных названий ингридиентов, в процессе можно видеть, что вначале модель периодически зацикливает возвращаемые символы, а после обучения можно видеть наборы символов более похожие на слова."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
